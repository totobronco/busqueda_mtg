import requests
from bs4 import BeautifulSoup
import csv
import time
from datetime import datetime
import os
import re

BASE_START = "https://www.huntercardtcg.com/categoria-producto/mtg/mtg-singles/page/{}/"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (compatible; scraping-script/1.0; +https://example.com)"
}

def preguntar_si_no(pregunta):
    """Pide confirmaci√≥n S/N hasta recibir una respuesta v√°lida"""
    while True:
        try:
            r = input(f"{pregunta} (S/N): ").strip().lower()
        except EOFError:
            return False
        if r in ("s", "n"):
            return r == "s"
        print("‚ùå Opci√≥n no v√°lida. Por favor, ingresa 'S' para s√≠ o 'N' para no.")

def formatear_moneda_clp(valor_int):
    """Formato $xx.xxx CLP (usa punto como separador de miles)."""
    if valor_int is None:
        return ""
    s = f"{valor_int:,}"          # produce "84,600"
    s = s.replace(",", ".")       # cambia a "84.600"
    return f"${s} CLP"

def limpiar_nombre(titulo):
    """
    Limpia el t√≠tulo:
    - Detecta si contiene 'foil' (case-insensitive)
    - Elimina par√©ntesis y su contenido
    - Toma la parte antes de '‚Äì' o '-' si existe
    - Devuelve nombre limpio y booleano foil
    """
    if not titulo:
        return "", False

    # Detectar foil en cualquier parte
    foil = bool(re.search(r"\bfoil\b", titulo, flags=re.I))

    # Eliminar contenido entre par√©ntesis
    s = re.sub(r"\s*\([^)]*\)", "", titulo)

    # Si hay guion largo '‚Äì' o guion normal '-', separar y tomar la primera parte
    s = re.split(r"\s+[‚Äì-]\s+", s)[0]

    # Limpiar espacios sobrantes y caracteres raros al inicio/final
    s = s.strip()

    # Opcional: eliminar etiquetas extra como #75, Common, Ingl√©s si quedaron con comas o guiones
    # Tomamos s√≥lo la primera parte por si a√∫n quedan ' ‚Äì ' u otros separadores
    return s, foil

def extraer_precio(texto_precio):
    """
    Extrae cifra num√©rica desde texto tipo "$490" o "$1.234" etc.
    Retorna entero en CLP (ej: 490) o None si falla.
    """
    if not texto_precio:
        return None
    # Extraer s√≥lo d√≠gitos (y posibles separadores), despu√©s convertir
    # Primero reemplazar posibles puntos/commas por vac√≠o
    # Pero para no romper n√∫meros con miles, extraemos todos los d√≠gitos y juntamos
    digits = re.findall(r"\d+", texto_precio)
    if not digits:
        return None
    number = "".join(digits)  # e.g. ["84", "600"] -> "84600" or ["490"] -> "490"
    try:
        return int(number)
    except ValueError:
        return None

def obtener_productos_por_pagina(numero_pagina):
    url = BASE_START.format(numero_pagina)
    resp = requests.get(url, headers=HEADERS, timeout=15)
    if resp.status_code != 200:
        # Si p√°gina no existe o problema, devolvemos lista vac√≠a
        return []

    soup = BeautifulSoup(resp.text, "html.parser")
    contenedores = soup.find_all("div", class_="thunk-product")
    resultados = []

    for cont in contenedores:
        # Elemento link con t√≠tulo
        a_tag = cont.find("a", class_="woocommerce-LoopProduct-link")
        titulo_tag = cont.find("h2", class_="woocommerce-loop-product__title")
        if not a_tag or not titulo_tag:
            continue

        titulo_raw = titulo_tag.get_text(strip=True)
        url_producto = a_tag.get("href")
        if url_producto and url_producto.startswith("/"):
            url_producto = "https://www.huntercardtcg.com" + url_producto

        # Imagen: buscar <img> dentro del a_tag o dentro de thunk-product-image
        img_tag = cont.find("img")
        img_url = None
        if img_tag:
            img_url = img_tag.get("data-src") or img_tag.get("src")
            if img_url and img_url.startswith("//"):
                img_url = "https:" + img_url

        # Precio: buscar span con clase 'woocommerce-Price-amount' o 'price'
        precio_text = ""
        price_span = cont.find("span", class_="woocommerce-Price-amount")
        if price_span:
            precio_text = price_span.get_text(strip=True)
        else:
            # fallback: buscar .price
            price_wrapper = cont.find("span", class_="price")
            if price_wrapper:
                precio_text = price_wrapper.get_text(strip=True)

        precio_int = extraer_precio(precio_text)
        precio_fmt = formatear_moneda_clp(precio_int) if precio_int is not None else ""

        nombre_limpio, es_foil = limpiar_nombre(titulo_raw)

        resultados.append({
            "nombre_original": titulo_raw,
            "nombre": nombre_limpio,
            "foil": "S√≠" if es_foil else "No",
            "precio": precio_fmt,
            "url": url_producto,
            "imagen": img_url
        })

    return resultados

def descargar_imagenes(productos, carpeta):
    os.makedirs(carpeta, exist_ok=True)
    total = len(productos)
    descargadas = 0
    for i, p in enumerate(productos, start=1):
        url_img = p.get("imagen")
        if not url_img:
            continue
        # Crear nombre de archivo seguro
        safe_name = re.sub(r"[^A-Za-z0-9 _-]", "", p["nombre"]).strip()
        safe_name = re.sub(r"\s+", "_", safe_name)[:80]
        ext = os.path.splitext(url_img)[1].split("?")[0]
        if not ext:
            ext = ".jpg"
        filename = f"{safe_name}{ext}"
        ruta = os.path.join(carpeta, filename)
        try:
            r = requests.get(url_img, headers=HEADERS, timeout=12)
            if r.status_code == 200:
                with open(ruta, "wb") as f:
                    f.write(r.content)
                descargadas += 1
                print(f"üì∏ ({i}/{total}) Guardada: {filename}")
            else:
                print(f"‚ö†Ô∏è ({i}/{total}) No se pudo bajar imagen (status {r.status_code}): {url_img}")
        except Exception as e:
            print(f"‚ö†Ô∏è ({i}/{total}) Error al bajar imagen: {e}")
        time.sleep(0.2)
    print(f"‚úÖ Im√°genes descargadas: {descargadas}/{total}")

def main():
    # Pregunta inicial para iniciar scraping
    if not preguntar_si_no("¬øDeseas iniciar el scraping de HunterCardTCG?"):
        print("‚úã Operaci√≥n cancelada por el usuario.")
        return

    pagina = 1
    todos = []
    print("üîé Iniciando scraping...")

    while True:
        print(f"‚û°Ô∏è Procesando p√°gina {pagina} ...")
        productos = obtener_productos_por_pagina(pagina)
        if not productos:
            print("‚ÑπÔ∏è No se encontraron productos en esta p√°gina ‚Äî deteniendo.")
            break
        todos.extend(productos)
        pagina += 1
        time.sleep(1)  # pausa para no sobrecargar

    # Crear carpeta y archivo CSV
    os.makedirs("Ficheros", exist_ok=True)
    fecha = datetime.now().strftime("%Y%m%d_%H%M")
    nombre_archivo = f"Ficheros/List_HunterCard_{fecha}.csv"

    with open(nombre_archivo, "w", newline="", encoding="utf-8") as csvfile:
        campos = ["nombre_original", "nombre", "foil", "precio", "url", "imagen"]
        writer = csv.DictWriter(csvfile, fieldnames=campos)
        writer.writeheader()
        writer.writerows(todos)

    print(f"\nüü¢ Proceso completado. Se guardaron {len(todos)} productos.")
    print(f"üìÅ Archivo CSV: {nombre_archivo}\n")

    # Preguntar si desea descargar im√°genes
    if preguntar_si_no("¬øDeseas descargar las im√°genes ahora?"):
        carpeta_img = "Ficheros/imagenes_hunter"
        descargar_imagenes(todos, carpeta_img)
    else:
        print("üö´ Descarga de im√°genes omitida por el usuario.")

if __name__ == "__main__":
    main()
